{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f38ae1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85f03ba",
   "metadata": {},
   "source": [
    "FC layer는 1차원 데이터를 사용하기에, 데이터의 형상이 무시된다는 문제가 발생.  \n",
    "이를 방지하고자 합성곱 계층을 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b44260c",
   "metadata": {},
   "source": [
    "### 7.4.2 img를 2차원 데이터로 전개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b56b456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원 배열\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97a88156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1, 3, 7, 7) # (데이터 수, 채널 수, 높이, 너비)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7) # (데이터 수, 채널 수, 높이, 너비)\n",
    "col1 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape) # 2번째 차원의 원소는 필터의 원소 개수와 동일함 (채널 x 필터 높이 x 필터 너비)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2386a1",
   "metadata": {},
   "source": [
    "### 7.4.3 합성곱 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55a33c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : 2차원 배열(입력 데이터)\n",
    "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img : 변환된 이미지들\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94da1fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 중간 데이터（backward 시 사용）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 기울기\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad) \n",
    "        col_W = self.W.reshape(FN, -1).T # 가중치 곱을 위해서 filter도 2차원으로 바꿔줌 (필터 요소 x 데이터 수)\n",
    "        out = np.dot(col, col_W) + self.b \n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0,3,1,2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN) # reshape(N, out_h, out_w, -1).transpose(0,3,1,2)의 역전파\n",
    "\n",
    "        self.db = np.sum(dout, axis=0) \n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad) # im2col의 역전파\n",
    " \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a145975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n750개 원소를 reshape(10, -1)로 하면 (10, 75)로 바뀜\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "750개 원소를 reshape(10, -1)로 하면 (10, 75)로 바뀜\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b834ee",
   "metadata": {},
   "source": [
    "### 7.4.4. 풀링 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d648f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w) # 1차원이 풀링 필터 요소 개수로 되도록 전개\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1) # transpose(0,3,1,2)의 역전파\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten() # reshape(N,out_h,out_w,C)의 역전파\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) # max(axis=1)의 역전파\n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1) # resape(-1, self.pool_h*self.pool_w)의 역전파\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad) # im2col 역전파\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e6244",
   "metadata": {},
   "source": [
    "## 7.5 CNN 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "295803c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv - relu - pooling - affine - relu - affine - softmax로 이뤄진 CNN\n",
    "\n",
    "from common.layers import Relu, Affine, SoftmaxWithLoss\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], \n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8c97820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299261162639016\n",
      "=== epoch:1, train acc:0.264, test acc:0.198 ===\n",
      "train loss:2.2972198908773183\n",
      "train loss:2.291458495184125\n",
      "train loss:2.284905386724968\n",
      "train loss:2.2782150359633966\n",
      "train loss:2.265129593130952\n",
      "train loss:2.2458383502281953\n",
      "train loss:2.229830438761598\n",
      "train loss:2.199664617235257\n",
      "train loss:2.166672400714106\n",
      "train loss:2.1587269187878984\n",
      "train loss:2.121006574408242\n",
      "train loss:2.0580520391231945\n",
      "train loss:1.9905761988068977\n",
      "train loss:1.9497228114378082\n",
      "train loss:1.8182708471666142\n",
      "train loss:1.8288805037340612\n",
      "train loss:1.6528295036387528\n",
      "train loss:1.6371102217950169\n",
      "train loss:1.6897945922911588\n",
      "train loss:1.4195920210578694\n",
      "train loss:1.3954818587259785\n",
      "train loss:1.3399280062364887\n",
      "train loss:1.2663940636554663\n",
      "train loss:1.0253411200516005\n",
      "train loss:0.9884892417573328\n",
      "train loss:1.1773858132375365\n",
      "train loss:0.9502726281815558\n",
      "train loss:1.0173047371319113\n",
      "train loss:0.9023421966330193\n",
      "train loss:0.8424883801591618\n",
      "train loss:0.7573778610048346\n",
      "train loss:0.7816391823416596\n",
      "train loss:0.804039248137107\n",
      "train loss:0.8236072505724681\n",
      "train loss:0.8652025675992373\n",
      "train loss:0.6197078650852563\n",
      "train loss:0.6936347261736538\n",
      "train loss:0.5436648617946863\n",
      "train loss:0.6350435947226761\n",
      "train loss:0.5690235051838837\n",
      "train loss:0.591196967402473\n",
      "train loss:0.5676827929221174\n",
      "train loss:0.5992077287062209\n",
      "train loss:0.6746080655307115\n",
      "train loss:0.5379711770058982\n",
      "train loss:0.5459521602470196\n",
      "train loss:0.4353048749141597\n",
      "train loss:0.3532906066324387\n",
      "train loss:0.5548946626253783\n",
      "train loss:0.5735719904305788\n",
      "=== epoch:2, train acc:0.843, test acc:0.799 ===\n",
      "train loss:0.5065969049986964\n",
      "train loss:0.6190222530345162\n",
      "train loss:0.44034840176861273\n",
      "train loss:0.6811745325028572\n",
      "train loss:0.6633405117828974\n",
      "train loss:0.44879442727237423\n",
      "train loss:0.4934189045474796\n",
      "train loss:0.48176349289199133\n",
      "train loss:0.3908120505814024\n",
      "train loss:0.5027183976962892\n",
      "train loss:0.5740556364843814\n",
      "train loss:0.450232573320137\n",
      "train loss:0.502957429235626\n",
      "train loss:0.34764361811308797\n",
      "train loss:0.3831422070897257\n",
      "train loss:0.31729480043561353\n",
      "train loss:0.5517272539696875\n",
      "train loss:0.60693935547109\n",
      "train loss:0.48654960185294527\n",
      "train loss:0.25912646120316174\n",
      "train loss:0.4139757625334194\n",
      "train loss:0.3893719895108049\n",
      "train loss:0.3953220480431574\n",
      "train loss:0.24780004714725706\n",
      "train loss:0.37398980988892233\n",
      "train loss:0.3916499206759625\n",
      "train loss:0.2622630538779072\n",
      "train loss:0.47344283816513505\n",
      "train loss:0.32213058481403933\n",
      "train loss:0.3274863878482168\n",
      "train loss:0.34437021191718536\n",
      "train loss:0.3345877756092955\n",
      "train loss:0.4344541287931481\n",
      "train loss:0.3519519203069085\n",
      "train loss:0.41309690561331647\n",
      "train loss:0.38693117456626624\n",
      "train loss:0.3077443944066116\n",
      "train loss:0.3930594077904334\n",
      "train loss:0.3582208596267405\n",
      "train loss:0.38918204327952394\n",
      "train loss:0.248722504925024\n",
      "train loss:0.36452246377056224\n",
      "train loss:0.3388535770219655\n",
      "train loss:0.2670717944814321\n",
      "train loss:0.3475692892123589\n",
      "train loss:0.2643710190590845\n",
      "train loss:0.5082682299414363\n",
      "train loss:0.13287084112016156\n",
      "train loss:0.284736430732154\n",
      "train loss:0.22137940157676872\n",
      "=== epoch:3, train acc:0.866, test acc:0.872 ===\n",
      "train loss:0.2556777078260829\n",
      "train loss:0.2283028766989011\n",
      "train loss:0.3490456928719756\n",
      "train loss:0.3494943906935882\n",
      "train loss:0.2574781636985835\n",
      "train loss:0.34496869125335294\n",
      "train loss:0.4622887723358346\n",
      "train loss:0.2762472712067968\n",
      "train loss:0.31459658825259773\n",
      "train loss:0.4033098929260748\n",
      "train loss:0.40765968826916255\n",
      "train loss:0.29790178462060624\n",
      "train loss:0.3977876752749335\n",
      "train loss:0.5282687538187157\n",
      "train loss:0.2557380636069359\n",
      "train loss:0.30616456980052326\n",
      "train loss:0.28725501430028005\n",
      "train loss:0.2859514484506134\n",
      "train loss:0.3576149551443955\n",
      "train loss:0.18982953702299146\n",
      "train loss:0.2072084818052311\n",
      "train loss:0.26893573295342255\n",
      "train loss:0.24089589599592404\n",
      "train loss:0.19281205854822037\n",
      "train loss:0.1792724065979812\n",
      "train loss:0.23144079229562983\n",
      "train loss:0.29689440182158583\n",
      "train loss:0.20244849554669944\n",
      "train loss:0.2666137042904097\n",
      "train loss:0.20521388225397186\n",
      "train loss:0.284759514766037\n",
      "train loss:0.25172373523456687\n",
      "train loss:0.28650694508188695\n",
      "train loss:0.37927307189419346\n",
      "train loss:0.2419467935404466\n",
      "train loss:0.40079218252171833\n",
      "train loss:0.4272762470733184\n",
      "train loss:0.2696730933277283\n",
      "train loss:0.22111635779606206\n",
      "train loss:0.24950044787003176\n",
      "train loss:0.3706291585606343\n",
      "train loss:0.285511152820728\n",
      "train loss:0.14347855589044609\n",
      "train loss:0.23416930534585426\n",
      "train loss:0.26224394867230605\n",
      "train loss:0.2993473891946046\n",
      "train loss:0.21155946711539042\n",
      "train loss:0.2218922997210238\n",
      "train loss:0.37131074308329753\n",
      "train loss:0.2991248770001677\n",
      "=== epoch:4, train acc:0.903, test acc:0.89 ===\n",
      "train loss:0.3691222815093467\n",
      "train loss:0.18649791580888003\n",
      "train loss:0.2756546857346343\n",
      "train loss:0.3048638277963787\n",
      "train loss:0.2836198359133342\n",
      "train loss:0.33005267137618055\n",
      "train loss:0.13997981411180144\n",
      "train loss:0.35583699316332584\n",
      "train loss:0.26444821384061146\n",
      "train loss:0.3313133489310291\n",
      "train loss:0.29162094569829233\n",
      "train loss:0.21659129505042396\n",
      "train loss:0.3935473535298397\n",
      "train loss:0.19003024566679547\n",
      "train loss:0.14139134142529078\n",
      "train loss:0.1927505537715314\n",
      "train loss:0.33204572971359325\n",
      "train loss:0.2540059542015379\n",
      "train loss:0.2164167803516881\n",
      "train loss:0.12585644801750426\n",
      "train loss:0.2700211349910696\n",
      "train loss:0.4167410439013255\n",
      "train loss:0.15077186580429525\n",
      "train loss:0.23525147400702204\n",
      "train loss:0.31424079582888803\n",
      "train loss:0.1682649022282047\n",
      "train loss:0.22778457128177843\n",
      "train loss:0.27366465072875046\n",
      "train loss:0.13061623707375133\n",
      "train loss:0.1765704204541801\n",
      "train loss:0.23297478925533566\n",
      "train loss:0.15043183990046624\n",
      "train loss:0.21632071272677833\n",
      "train loss:0.19043040772692965\n",
      "train loss:0.15152878713243592\n",
      "train loss:0.299278839279813\n",
      "train loss:0.24649814825823846\n",
      "train loss:0.20788668095410462\n",
      "train loss:0.34149839128703463\n",
      "train loss:0.21463416162212026\n",
      "train loss:0.12697788733142992\n",
      "train loss:0.1701769429405724\n",
      "train loss:0.16090447389277482\n",
      "train loss:0.14006091675918247\n",
      "train loss:0.25918449284689\n",
      "train loss:0.2676955974186763\n",
      "train loss:0.30876229967346275\n",
      "train loss:0.25830999258677556\n",
      "train loss:0.3765854310043084\n",
      "train loss:0.31437717335252513\n",
      "=== epoch:5, train acc:0.925, test acc:0.91 ===\n",
      "train loss:0.18107520995202042\n",
      "train loss:0.2728903950799027\n",
      "train loss:0.2111933424894753\n",
      "train loss:0.23650980591602536\n",
      "train loss:0.23296981487104376\n",
      "train loss:0.1377377876392051\n",
      "train loss:0.24342234215892078\n",
      "train loss:0.14998962942421148\n",
      "train loss:0.17939263824537874\n",
      "train loss:0.22337094975711316\n",
      "train loss:0.20927541980437683\n",
      "train loss:0.14348788224431208\n",
      "train loss:0.1669247729517041\n",
      "train loss:0.18739015382144816\n",
      "train loss:0.13109753546325914\n",
      "train loss:0.11957803477230416\n",
      "train loss:0.21765627324327777\n",
      "train loss:0.23969569701744622\n",
      "train loss:0.2145458928995865\n",
      "train loss:0.24810221540639957\n",
      "train loss:0.2554123564917628\n",
      "train loss:0.23073677721535246\n",
      "train loss:0.27118089187984007\n",
      "train loss:0.08148025812868799\n",
      "train loss:0.2696180360143981\n",
      "train loss:0.12444390909639756\n",
      "train loss:0.12842548550436297\n",
      "train loss:0.21074566469146092\n",
      "train loss:0.1545011393681918\n",
      "train loss:0.29658715948430425\n",
      "train loss:0.22650541408967415\n",
      "train loss:0.2363297172561518\n",
      "train loss:0.163827204753855\n",
      "train loss:0.15406565324244448\n",
      "train loss:0.2054735669079756\n",
      "train loss:0.19056601485231472\n",
      "train loss:0.284651520777657\n",
      "train loss:0.16127162096271402\n",
      "train loss:0.2164225237497238\n",
      "train loss:0.12484245545248568\n",
      "train loss:0.21314296785978915\n",
      "train loss:0.17591623182492205\n",
      "train loss:0.21522367756968605\n",
      "train loss:0.21864178121191077\n",
      "train loss:0.15078046622588637\n",
      "train loss:0.1986779256152256\n",
      "train loss:0.16644005719594301\n",
      "train loss:0.11809136104393182\n",
      "train loss:0.12549435880668303\n",
      "train loss:0.145294322424448\n",
      "=== epoch:6, train acc:0.942, test acc:0.922 ===\n",
      "train loss:0.12686929389198745\n",
      "train loss:0.06340346295749474\n",
      "train loss:0.20479847093783227\n",
      "train loss:0.12843017156962863\n",
      "train loss:0.16301702109516797\n",
      "train loss:0.1206919498405373\n",
      "train loss:0.17677718875328766\n",
      "train loss:0.2403935618715883\n",
      "train loss:0.09633229562917363\n",
      "train loss:0.12730087164842888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.22076746475771586\n",
      "train loss:0.17838887766069086\n",
      "train loss:0.10323273900496037\n",
      "train loss:0.2494008000896881\n",
      "train loss:0.21821350821597915\n",
      "train loss:0.26779940244254624\n",
      "train loss:0.22832315359570377\n",
      "train loss:0.1708984904297204\n",
      "train loss:0.15181682735201765\n",
      "train loss:0.1095612371690088\n",
      "train loss:0.18644248491273152\n",
      "train loss:0.13047768534102028\n",
      "train loss:0.1307224686746123\n",
      "train loss:0.14717816791703536\n",
      "train loss:0.1911727922428117\n",
      "train loss:0.2723507547252852\n",
      "train loss:0.1409380116623724\n",
      "train loss:0.1369571352534098\n",
      "train loss:0.10901401596023032\n",
      "train loss:0.2826508616004925\n",
      "train loss:0.17345196191806805\n",
      "train loss:0.16718924573364935\n",
      "train loss:0.2664420540121654\n",
      "train loss:0.09402343696751807\n",
      "train loss:0.18973751999540597\n",
      "train loss:0.12004455190737966\n",
      "train loss:0.12215987544109874\n",
      "train loss:0.1303835863549244\n",
      "train loss:0.14722000573292074\n",
      "train loss:0.17872997872881566\n",
      "train loss:0.23028745660129588\n",
      "train loss:0.13852657545854094\n",
      "train loss:0.14824259673003723\n",
      "train loss:0.12388999311690423\n",
      "train loss:0.2123019539848608\n",
      "train loss:0.09134487002740949\n",
      "train loss:0.14782256106174008\n",
      "train loss:0.28059256152069967\n",
      "train loss:0.16273739798238013\n",
      "train loss:0.1134793103125757\n",
      "=== epoch:7, train acc:0.94, test acc:0.929 ===\n",
      "train loss:0.10244310242359328\n",
      "train loss:0.09725011116650967\n",
      "train loss:0.24426235199272292\n",
      "train loss:0.10141391416062737\n",
      "train loss:0.15915442248250844\n",
      "train loss:0.07374478228224929\n",
      "train loss:0.12050946722536049\n",
      "train loss:0.12441017804905005\n",
      "train loss:0.20010424675510344\n",
      "train loss:0.10389811199033785\n",
      "train loss:0.12177999447116707\n",
      "train loss:0.12368714117520468\n",
      "train loss:0.0953549234615986\n",
      "train loss:0.18777390431119073\n",
      "train loss:0.15998622832481174\n",
      "train loss:0.09006290704745792\n",
      "train loss:0.09236061323612312\n",
      "train loss:0.10480992784251568\n",
      "train loss:0.18089488518967173\n",
      "train loss:0.1274871415991437\n",
      "train loss:0.06101787335710766\n",
      "train loss:0.07659473963758902\n",
      "train loss:0.15184026113104052\n",
      "train loss:0.07934792372536942\n",
      "train loss:0.1450401445887551\n",
      "train loss:0.14137099680401893\n",
      "train loss:0.15071845296029918\n",
      "train loss:0.13103770999770062\n",
      "train loss:0.0998858804379938\n",
      "train loss:0.06438004592671395\n",
      "train loss:0.1774168670109514\n",
      "train loss:0.08706136758420058\n",
      "train loss:0.07798418295202128\n",
      "train loss:0.11449475469850619\n",
      "train loss:0.136572068055087\n",
      "train loss:0.18374097638684878\n",
      "train loss:0.08339557541192016\n",
      "train loss:0.13365385102796384\n",
      "train loss:0.0640325575672129\n",
      "train loss:0.06503052116194072\n",
      "train loss:0.07729275595445573\n",
      "train loss:0.15371061901919725\n",
      "train loss:0.11684401694259712\n",
      "train loss:0.08223900029964938\n",
      "train loss:0.13463445660276052\n",
      "train loss:0.10532668393774607\n",
      "train loss:0.13215168500709307\n",
      "train loss:0.04822551838756023\n",
      "train loss:0.10468157911472786\n",
      "train loss:0.09583397356711196\n",
      "=== epoch:8, train acc:0.95, test acc:0.936 ===\n",
      "train loss:0.06768967580044016\n",
      "train loss:0.20018764779300519\n",
      "train loss:0.1508418272762475\n",
      "train loss:0.0814237306772882\n",
      "train loss:0.062340109291434846\n",
      "train loss:0.13499106585771037\n",
      "train loss:0.09932953921012184\n",
      "train loss:0.09429567371597791\n",
      "train loss:0.09628960560291969\n",
      "train loss:0.1423841539133517\n",
      "train loss:0.09947823735295364\n",
      "train loss:0.1383453842267713\n",
      "train loss:0.2695822262884467\n",
      "train loss:0.1745938104829985\n",
      "train loss:0.1448271962900439\n",
      "train loss:0.10153943238761105\n",
      "train loss:0.0965066235615097\n",
      "train loss:0.12637871296124867\n",
      "train loss:0.15670792969475789\n",
      "train loss:0.0891700805303665\n",
      "train loss:0.13631723213896632\n",
      "train loss:0.07875217826093739\n",
      "train loss:0.11680433491882672\n",
      "train loss:0.12095445332432954\n",
      "train loss:0.08581617714599635\n",
      "train loss:0.09360974470719523\n",
      "train loss:0.1349641659227557\n",
      "train loss:0.14542692054311618\n",
      "train loss:0.15095523738611058\n",
      "train loss:0.11073903473521358\n",
      "train loss:0.09007180928029422\n",
      "train loss:0.12666184882180737\n",
      "train loss:0.08313236075179241\n",
      "train loss:0.12124752553653823\n",
      "train loss:0.11753721020807649\n",
      "train loss:0.11230978078967169\n",
      "train loss:0.13253603817599444\n",
      "train loss:0.19663354094046734\n",
      "train loss:0.18992142399425152\n",
      "train loss:0.15631498204242938\n",
      "train loss:0.18423617042600335\n",
      "train loss:0.08269207152898593\n",
      "train loss:0.1053807736872384\n",
      "train loss:0.18360606324570714\n",
      "train loss:0.09541985079361563\n",
      "train loss:0.09963000644248188\n",
      "train loss:0.10070340200383456\n",
      "train loss:0.12803500641966994\n",
      "train loss:0.06475359925710122\n",
      "train loss:0.05760248706744714\n",
      "=== epoch:9, train acc:0.961, test acc:0.933 ===\n",
      "train loss:0.08456641675512042\n",
      "train loss:0.08350449779571123\n",
      "train loss:0.07773803727812463\n",
      "train loss:0.0966728744951689\n",
      "train loss:0.04817764667035756\n",
      "train loss:0.12892764486239403\n",
      "train loss:0.06072431935650986\n",
      "train loss:0.08869562749661909\n",
      "train loss:0.09137611596772127\n",
      "train loss:0.12477830058289084\n",
      "train loss:0.12260300126198073\n",
      "train loss:0.10209002983419295\n",
      "train loss:0.2505164801783145\n",
      "train loss:0.07021976210698756\n",
      "train loss:0.06506143498225866\n",
      "train loss:0.1121757201868225\n",
      "train loss:0.046244872990459936\n",
      "train loss:0.11775995117047808\n",
      "train loss:0.12359656438812498\n",
      "train loss:0.12758967048169226\n",
      "train loss:0.08173626182163575\n",
      "train loss:0.18562201267471504\n",
      "train loss:0.08090318510974441\n",
      "train loss:0.07991011164368836\n",
      "train loss:0.10058137419022899\n",
      "train loss:0.08881110599793635\n",
      "train loss:0.0598997580587025\n",
      "train loss:0.06262318721860793\n",
      "train loss:0.15161853758935437\n",
      "train loss:0.08453493053114984\n",
      "train loss:0.037261898697766284\n",
      "train loss:0.11588487592190898\n",
      "train loss:0.24315009463296172\n",
      "train loss:0.12338564101485884\n",
      "train loss:0.10153678298508538\n",
      "train loss:0.06783956561290877\n",
      "train loss:0.1055246605014743\n",
      "train loss:0.10645786211454597\n",
      "train loss:0.05000516453454866\n",
      "train loss:0.11103262416718206\n",
      "train loss:0.11585066942505212\n",
      "train loss:0.08084744254696949\n",
      "train loss:0.056874477859978574\n",
      "train loss:0.09464310054807344\n",
      "train loss:0.0739677375249704\n",
      "train loss:0.03569190190846029\n",
      "train loss:0.052804197901143796\n",
      "train loss:0.09886974725953612\n",
      "train loss:0.1465368116536946\n",
      "train loss:0.060226653618708846\n",
      "=== epoch:10, train acc:0.965, test acc:0.946 ===\n",
      "train loss:0.06418175987543197\n",
      "train loss:0.1169029621050667\n",
      "train loss:0.06002354285115545\n",
      "train loss:0.09710039013654176\n",
      "train loss:0.03985267049775932\n",
      "train loss:0.1030411569820857\n",
      "train loss:0.05502462115000177\n",
      "train loss:0.0937443425336228\n",
      "train loss:0.18053092893255707\n",
      "train loss:0.11806836469960505\n",
      "train loss:0.05721168227702338\n",
      "train loss:0.08974989493040322\n",
      "train loss:0.06262682678587768\n",
      "train loss:0.04760387858814692\n",
      "train loss:0.057325211992718374\n",
      "train loss:0.05159806315111614\n",
      "train loss:0.1315751309029798\n",
      "train loss:0.08323115430041593\n",
      "train loss:0.05458988163542603\n",
      "train loss:0.09677599040461313\n",
      "train loss:0.09613516422566595\n",
      "train loss:0.11322600063471705\n",
      "train loss:0.15715644374495827\n",
      "train loss:0.07469386054203231\n",
      "train loss:0.11594837111407358\n",
      "train loss:0.05259990260927438\n",
      "train loss:0.06805987986957998\n",
      "train loss:0.05405313591768559\n",
      "train loss:0.11986014699003125\n",
      "train loss:0.07168895724855978\n",
      "train loss:0.06609640179863692\n",
      "train loss:0.06098351353358946\n",
      "train loss:0.0708256454220818\n",
      "train loss:0.0877371178488425\n",
      "train loss:0.06999918390352688\n",
      "train loss:0.03631274219759788\n",
      "train loss:0.046562971919056916\n",
      "train loss:0.06172404712739324\n",
      "train loss:0.08745808688818073\n",
      "train loss:0.12072898931723408\n",
      "train loss:0.05194661787745866\n",
      "train loss:0.08227096192282105\n",
      "train loss:0.04316268684724222\n",
      "train loss:0.12383515372841149\n",
      "train loss:0.04790976658459923\n",
      "train loss:0.056376521868161736\n",
      "train loss:0.14580796095410173\n",
      "train loss:0.041950236508760036\n",
      "train loss:0.08780978627331333\n",
      "train loss:0.06355161953384789\n",
      "=== epoch:11, train acc:0.97, test acc:0.949 ===\n",
      "train loss:0.10546253750759046\n",
      "train loss:0.034832569917585314\n",
      "train loss:0.04319601851706504\n",
      "train loss:0.055510806881200914\n",
      "train loss:0.15667411368940704\n",
      "train loss:0.06902427268841956\n",
      "train loss:0.06343588712703845\n",
      "train loss:0.061113678934499774\n",
      "train loss:0.05536556325572237\n",
      "train loss:0.07303201872404659\n",
      "train loss:0.06298486853058867\n",
      "train loss:0.05974674905825186\n",
      "train loss:0.10264454890900661\n",
      "train loss:0.03970657639436135\n",
      "train loss:0.06401845991733342\n",
      "train loss:0.14450204941789305\n",
      "train loss:0.08661679669886028\n",
      "train loss:0.096661000039936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.08365697151548593\n",
      "train loss:0.03793421149184588\n",
      "train loss:0.06666407925608488\n",
      "train loss:0.044904112397189874\n",
      "train loss:0.05403269172098305\n",
      "train loss:0.11647538061850476\n",
      "train loss:0.11217928924966193\n",
      "train loss:0.033342145556756006\n",
      "train loss:0.11943996659241407\n",
      "train loss:0.1071605592460473\n",
      "train loss:0.04013525956655489\n",
      "train loss:0.0690220473777111\n",
      "train loss:0.10025835632476295\n",
      "train loss:0.08042507762149828\n",
      "train loss:0.048962497432033424\n",
      "train loss:0.09624355601342653\n",
      "train loss:0.08178788880791464\n",
      "train loss:0.10811319895492076\n",
      "train loss:0.0800897008616611\n",
      "train loss:0.04166480750473829\n",
      "train loss:0.04337673819744545\n",
      "train loss:0.06851780260503884\n",
      "train loss:0.05726647963112171\n",
      "train loss:0.061953323337747085\n",
      "train loss:0.054839204260206886\n",
      "train loss:0.05634979486631346\n",
      "train loss:0.07222146647038963\n",
      "train loss:0.08647252932679052\n",
      "train loss:0.016820095413747696\n",
      "train loss:0.08943064831338923\n",
      "train loss:0.06884612954227765\n",
      "train loss:0.047616630644253144\n",
      "=== epoch:12, train acc:0.976, test acc:0.958 ===\n",
      "train loss:0.0898029056466609\n",
      "train loss:0.10274302181177283\n",
      "train loss:0.04437976823002528\n",
      "train loss:0.1498123554013428\n",
      "train loss:0.08832768264596956\n",
      "train loss:0.058921023999627266\n",
      "train loss:0.08769341282320048\n",
      "train loss:0.1032133900945237\n",
      "train loss:0.05382783339542756\n",
      "train loss:0.0409484164927929\n",
      "train loss:0.07179316253826559\n",
      "train loss:0.04772553293493543\n",
      "train loss:0.10475496092250576\n",
      "train loss:0.054723745467512706\n",
      "train loss:0.13425118187224477\n",
      "train loss:0.06651000707907197\n",
      "train loss:0.1612108078994738\n",
      "train loss:0.038237788704158795\n",
      "train loss:0.10667602269949895\n",
      "train loss:0.12975200219466715\n",
      "train loss:0.02459571607932822\n",
      "train loss:0.03175145235569063\n",
      "train loss:0.060410043014415826\n",
      "train loss:0.09571413533831384\n",
      "train loss:0.03410991846406495\n",
      "train loss:0.03198189132633485\n",
      "train loss:0.05232472229398814\n",
      "train loss:0.04828823670115065\n",
      "train loss:0.05080940370807048\n",
      "train loss:0.09007368252827039\n",
      "train loss:0.04354743671973732\n",
      "train loss:0.03837557464543329\n",
      "train loss:0.07604255675651275\n",
      "train loss:0.02400324160930244\n",
      "train loss:0.07078002175874949\n",
      "train loss:0.08793867942175931\n",
      "train loss:0.05743860111974353\n",
      "train loss:0.03832244223880538\n",
      "train loss:0.031043479779141223\n",
      "train loss:0.022212230121013504\n",
      "train loss:0.028067345403579892\n",
      "train loss:0.12385133383393729\n",
      "train loss:0.03607188734016714\n",
      "train loss:0.0613726284194437\n",
      "train loss:0.033922817713710446\n",
      "train loss:0.08898390075262252\n",
      "train loss:0.03604970792946085\n",
      "train loss:0.06476978350886378\n",
      "train loss:0.060187427869238856\n",
      "train loss:0.022592706114919446\n",
      "=== epoch:13, train acc:0.978, test acc:0.96 ===\n",
      "train loss:0.04497456271592792\n",
      "train loss:0.026389959170056848\n",
      "train loss:0.02759318374623568\n",
      "train loss:0.1118370207896724\n",
      "train loss:0.06325263994646617\n",
      "train loss:0.06806085070537278\n",
      "train loss:0.0494617476178901\n",
      "train loss:0.06063329551248539\n",
      "train loss:0.03534883929087826\n",
      "train loss:0.06179294126667476\n",
      "train loss:0.025206032747783796\n",
      "train loss:0.019390425512873916\n",
      "train loss:0.061259406338238946\n",
      "train loss:0.056797179982071386\n",
      "train loss:0.09941230808927594\n",
      "train loss:0.05731045392814352\n",
      "train loss:0.0363236607182367\n",
      "train loss:0.053187187010856736\n",
      "train loss:0.03604731687924997\n",
      "train loss:0.0587030480118723\n",
      "train loss:0.020193467183691458\n",
      "train loss:0.022223104249042214\n",
      "train loss:0.02996584622521206\n",
      "train loss:0.03888721967874789\n",
      "train loss:0.05936121729651614\n",
      "train loss:0.10006220854902076\n",
      "train loss:0.02815273991560999\n",
      "train loss:0.047631385965988295\n",
      "train loss:0.02237580206757986\n",
      "train loss:0.029448849320816747\n",
      "train loss:0.02961532016613548\n",
      "train loss:0.03703514086668936\n",
      "train loss:0.06216307668881438\n",
      "train loss:0.05057860811140355\n",
      "train loss:0.06883431881223563\n",
      "train loss:0.09009813241615897\n",
      "train loss:0.010421389537372823\n",
      "train loss:0.04314353832313147\n",
      "train loss:0.02949505971683958\n",
      "train loss:0.043446707424794374\n",
      "train loss:0.02981074119416059\n",
      "train loss:0.03251605193590925\n",
      "train loss:0.023840624641956736\n",
      "train loss:0.03599710453435432\n",
      "train loss:0.0891426494291254\n",
      "train loss:0.055268952037100555\n",
      "train loss:0.03569628394352869\n",
      "train loss:0.03270288378222107\n",
      "train loss:0.01971707851261686\n",
      "train loss:0.04161436016280016\n",
      "=== epoch:14, train acc:0.981, test acc:0.957 ===\n",
      "train loss:0.026227640828311943\n",
      "train loss:0.029093629362198423\n",
      "train loss:0.05748709625647924\n",
      "train loss:0.04420893628356404\n",
      "train loss:0.030531414060619644\n",
      "train loss:0.08193916582160381\n",
      "train loss:0.027734355879292724\n",
      "train loss:0.02338264641873188\n",
      "train loss:0.039214294511116514\n",
      "train loss:0.010099959229363386\n",
      "train loss:0.03333383523161845\n",
      "train loss:0.01592458468391543\n",
      "train loss:0.03594317783939311\n",
      "train loss:0.059141547959375436\n",
      "train loss:0.026468598425613856\n",
      "train loss:0.08240174373096643\n",
      "train loss:0.06700033219609251\n",
      "train loss:0.0649846611657024\n",
      "train loss:0.022031048262514293\n",
      "train loss:0.03888899113189172\n",
      "train loss:0.04778831019182255\n",
      "train loss:0.025485851875202407\n",
      "train loss:0.13597932390130804\n",
      "train loss:0.023825055237820948\n",
      "train loss:0.0842848362010217\n",
      "train loss:0.027126034087439316\n",
      "train loss:0.026841430495833832\n",
      "train loss:0.03055223953192349\n",
      "train loss:0.034219677351624365\n",
      "train loss:0.016456399578477162\n",
      "train loss:0.05964492407474851\n",
      "train loss:0.028736864940385236\n",
      "train loss:0.09496517686063337\n",
      "train loss:0.04053627141043356\n",
      "train loss:0.013647469865702975\n",
      "train loss:0.048439896703701295\n",
      "train loss:0.013885134899964142\n",
      "train loss:0.01656215943078334\n",
      "train loss:0.008235885744730043\n",
      "train loss:0.05235429443408715\n",
      "train loss:0.016997052413256705\n",
      "train loss:0.05520885692083684\n",
      "train loss:0.021616712083259427\n",
      "train loss:0.03857729712885516\n",
      "train loss:0.06806141981053107\n",
      "train loss:0.026324649708600177\n",
      "train loss:0.04693948710900646\n",
      "train loss:0.038173079148501314\n",
      "train loss:0.06220309030320022\n",
      "train loss:0.06748762971624345\n",
      "=== epoch:15, train acc:0.983, test acc:0.956 ===\n",
      "train loss:0.04271683490633671\n",
      "train loss:0.015503821981473204\n",
      "train loss:0.008687669566871649\n",
      "train loss:0.05193511539878306\n",
      "train loss:0.06603781318696572\n",
      "train loss:0.012101237228098811\n",
      "train loss:0.04151634935087496\n",
      "train loss:0.08150832616577426\n",
      "train loss:0.029443777617667366\n",
      "train loss:0.026954549367800347\n",
      "train loss:0.018502796782628797\n",
      "train loss:0.02835199418247214\n",
      "train loss:0.020207331818086708\n",
      "train loss:0.06975893884118614\n",
      "train loss:0.031187165356642656\n",
      "train loss:0.017243360088172045\n",
      "train loss:0.05319573819658421\n",
      "train loss:0.05219442047627563\n",
      "train loss:0.03959264348093002\n",
      "train loss:0.03138734752272188\n",
      "train loss:0.03488170038015566\n",
      "train loss:0.012566362046530439\n",
      "train loss:0.03520369403530182\n",
      "train loss:0.05168808542636201\n",
      "train loss:0.04384508477775548\n",
      "train loss:0.049665095023906966\n",
      "train loss:0.051039275400398945\n",
      "train loss:0.05040538407949167\n",
      "train loss:0.029327801419293163\n",
      "train loss:0.03984243712820615\n",
      "train loss:0.054209072762348744\n",
      "train loss:0.03716415226493325\n",
      "train loss:0.04747623924101151\n",
      "train loss:0.021538491355567353\n",
      "train loss:0.0236219156739629\n",
      "train loss:0.08906724987836612\n",
      "train loss:0.027067258489718946\n",
      "train loss:0.0866104189612894\n",
      "train loss:0.03193781805113858\n",
      "train loss:0.017950354192675586\n",
      "train loss:0.023326387070929152\n",
      "train loss:0.03531130283799344\n",
      "train loss:0.04528261819079158\n",
      "train loss:0.02357519786069478\n",
      "train loss:0.03563274616981547\n",
      "train loss:0.022740309452799926\n",
      "train loss:0.050150306452159\n",
      "train loss:0.022977162855699507\n",
      "train loss:0.023927074397604168\n",
      "train loss:0.028736584882741178\n",
      "=== epoch:16, train acc:0.983, test acc:0.957 ===\n",
      "train loss:0.023548352748855025\n",
      "train loss:0.020821355671254295\n",
      "train loss:0.07149494366424987\n",
      "train loss:0.051135034372064264\n",
      "train loss:0.014122742664685533\n",
      "train loss:0.011211249444952422\n",
      "train loss:0.023392331204881774\n",
      "train loss:0.04033540700852987\n",
      "train loss:0.05046507249312774\n",
      "train loss:0.02899561731675643\n",
      "train loss:0.015896954895364322\n",
      "train loss:0.024577337939729382\n",
      "train loss:0.021170921840732722\n",
      "train loss:0.042832050662975095\n",
      "train loss:0.0355563854569144\n",
      "train loss:0.03182387979559244\n",
      "train loss:0.018677896573956257\n",
      "train loss:0.013777494363949474\n",
      "train loss:0.012615803708941336\n",
      "train loss:0.01919488306312481\n",
      "train loss:0.013966441271405677\n",
      "train loss:0.028383503313166095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.04394353666394861\n",
      "train loss:0.06529294273532388\n",
      "train loss:0.060804220826357974\n",
      "train loss:0.020671685553297757\n",
      "train loss:0.039059327396966534\n",
      "train loss:0.027216710325048212\n",
      "train loss:0.04085099067308179\n",
      "train loss:0.04237182522653841\n",
      "train loss:0.03132903965807701\n",
      "train loss:0.06410928455312633\n",
      "train loss:0.04840781469739547\n",
      "train loss:0.03627855793187829\n",
      "train loss:0.040603525456161235\n",
      "train loss:0.02145061010739434\n",
      "train loss:0.013507834963783558\n",
      "train loss:0.03336010322094875\n",
      "train loss:0.013450602520935051\n",
      "train loss:0.06072106666329986\n",
      "train loss:0.08932301909521449\n",
      "train loss:0.03799287240251254\n",
      "train loss:0.026128442415078744\n",
      "train loss:0.014609324648671146\n",
      "train loss:0.05107195802593358\n",
      "train loss:0.015755760985535668\n",
      "train loss:0.04701005935452069\n",
      "train loss:0.07054941698528173\n",
      "train loss:0.03248111445797068\n",
      "train loss:0.01417382678216907\n",
      "=== epoch:17, train acc:0.986, test acc:0.957 ===\n",
      "train loss:0.06673382341544\n",
      "train loss:0.01899274054363904\n",
      "train loss:0.04601722959616575\n",
      "train loss:0.05057323255181936\n",
      "train loss:0.015025177258890896\n",
      "train loss:0.015073374853139905\n",
      "train loss:0.03304743012417315\n",
      "train loss:0.037639902845256694\n",
      "train loss:0.013675354540657728\n",
      "train loss:0.043868925274347485\n",
      "train loss:0.030208928509596493\n",
      "train loss:0.036073244826299346\n",
      "train loss:0.010926140060259675\n",
      "train loss:0.05945052211178086\n",
      "train loss:0.03271104342234926\n",
      "train loss:0.021151571443576444\n",
      "train loss:0.06876580817272265\n",
      "train loss:0.060802381733718984\n",
      "train loss:0.01700220691171124\n",
      "train loss:0.029908250737639676\n",
      "train loss:0.0229034767041019\n",
      "train loss:0.04379426921615462\n",
      "train loss:0.025911940327471895\n",
      "train loss:0.046432095061014866\n",
      "train loss:0.043360340546825374\n",
      "train loss:0.017666482139940945\n",
      "train loss:0.048617641113385526\n",
      "train loss:0.040560956347294806\n",
      "train loss:0.02828701534551601\n",
      "train loss:0.015076607601296674\n",
      "train loss:0.03933700436905108\n",
      "train loss:0.045939776737417407\n",
      "train loss:0.022919904188816807\n",
      "train loss:0.010517264816442473\n",
      "train loss:0.015322694119304987\n",
      "train loss:0.04572413104345102\n",
      "train loss:0.029395509791408956\n",
      "train loss:0.013230519029590078\n",
      "train loss:0.015700132280407855\n",
      "train loss:0.01422773341546981\n",
      "train loss:0.033268667161869755\n",
      "train loss:0.0207334481855862\n",
      "train loss:0.027076976547077098\n",
      "train loss:0.009975168164124365\n",
      "train loss:0.025657476534955467\n",
      "train loss:0.01611610806043782\n",
      "train loss:0.025631207354180643\n",
      "train loss:0.017384705989503983\n",
      "train loss:0.022632274724527602\n",
      "train loss:0.03471144109473176\n",
      "=== epoch:18, train acc:0.993, test acc:0.966 ===\n",
      "train loss:0.03292229467601023\n",
      "train loss:0.01943892564768345\n",
      "train loss:0.015223874091423466\n",
      "train loss:0.01588387975075813\n",
      "train loss:0.01910132361978557\n",
      "train loss:0.008932548411566545\n",
      "train loss:0.014255757013532187\n",
      "train loss:0.011675252075413927\n",
      "train loss:0.01764584301864654\n",
      "train loss:0.02502867444569338\n",
      "train loss:0.02542238700501317\n",
      "train loss:0.02318509930548661\n",
      "train loss:0.0136208523555268\n",
      "train loss:0.030968299627158923\n",
      "train loss:0.010406880788231489\n",
      "train loss:0.03882231523321285\n",
      "train loss:0.022298286876765574\n",
      "train loss:0.028868616381487668\n",
      "train loss:0.02945305426499795\n",
      "train loss:0.013153910559969563\n",
      "train loss:0.006229152244488133\n",
      "train loss:0.008627255880925137\n",
      "train loss:0.027633919414739624\n",
      "train loss:0.023021625514447107\n",
      "train loss:0.026201041632556835\n",
      "train loss:0.0127367254292859\n",
      "train loss:0.004657594863687782\n",
      "train loss:0.0369158640233946\n",
      "train loss:0.038618963739417075\n",
      "train loss:0.016397303854726094\n",
      "train loss:0.013767316600813708\n",
      "train loss:0.021037652278475934\n",
      "train loss:0.005975594751194051\n",
      "train loss:0.02407692805758149\n",
      "train loss:0.014402467832511897\n",
      "train loss:0.010607998163469856\n",
      "train loss:0.009462842990889455\n",
      "train loss:0.02124431623275905\n",
      "train loss:0.014832767370130078\n",
      "train loss:0.01130837450257307\n",
      "train loss:0.01108429683430626\n",
      "train loss:0.009121359421615799\n",
      "train loss:0.044928099189891366\n",
      "train loss:0.017105915425714734\n",
      "train loss:0.004923731744559956\n",
      "train loss:0.014507136136396915\n",
      "train loss:0.035758426494353845\n",
      "train loss:0.018243353739778805\n",
      "train loss:0.025829210728129538\n",
      "train loss:0.013706752234407657\n",
      "=== epoch:19, train acc:0.991, test acc:0.96 ===\n",
      "train loss:0.007792709118924922\n",
      "train loss:0.009895518088934501\n",
      "train loss:0.03232401396895129\n",
      "train loss:0.0071815404856618754\n",
      "train loss:0.011243626110762359\n",
      "train loss:0.01956581451391578\n",
      "train loss:0.018529747583760227\n",
      "train loss:0.03629671073627755\n",
      "train loss:0.019997075498674407\n",
      "train loss:0.031266060099028034\n",
      "train loss:0.01882829083232795\n",
      "train loss:0.00757164568833894\n",
      "train loss:0.024257405339542078\n",
      "train loss:0.023028146510447524\n",
      "train loss:0.011240419126969181\n",
      "train loss:0.03086278210888593\n",
      "train loss:0.038397263189952936\n",
      "train loss:0.008512298012804651\n",
      "train loss:0.011137268947237053\n",
      "train loss:0.01703346815013221\n",
      "train loss:0.02270832830395582\n",
      "train loss:0.014365795977951392\n",
      "train loss:0.01377476260201798\n",
      "train loss:0.006594643075543916\n",
      "train loss:0.008987456054138835\n",
      "train loss:0.010599474610517814\n",
      "train loss:0.029013474872687267\n",
      "train loss:0.010862170298435286\n",
      "train loss:0.01462438744922217\n",
      "train loss:0.0173983472070324\n",
      "train loss:0.007032541982139483\n",
      "train loss:0.006721298707806466\n",
      "train loss:0.012250004136602982\n",
      "train loss:0.011042435018079532\n",
      "train loss:0.007506083688724709\n",
      "train loss:0.01072051394543412\n",
      "train loss:0.015069695110110456\n",
      "train loss:0.0062473701002534136\n",
      "train loss:0.015463104948735009\n",
      "train loss:0.006536736146189846\n",
      "train loss:0.02572138528561664\n",
      "train loss:0.005689879347218954\n",
      "train loss:0.02984001784271565\n",
      "train loss:0.009864825645680203\n",
      "train loss:0.04664831324235755\n",
      "train loss:0.0120819103961651\n",
      "train loss:0.02927367965174959\n",
      "train loss:0.0055450542178554305\n",
      "train loss:0.01325223386866811\n",
      "train loss:0.01242839703893107\n",
      "=== epoch:20, train acc:0.996, test acc:0.959 ===\n",
      "train loss:0.006907892181762296\n",
      "train loss:0.03107243884949175\n",
      "train loss:0.018686367974294443\n",
      "train loss:0.01506326262636123\n",
      "train loss:0.015096729421023625\n",
      "train loss:0.01938191279504764\n",
      "train loss:0.004463435832927919\n",
      "train loss:0.018123216572202918\n",
      "train loss:0.006917000261950611\n",
      "train loss:0.01179639902159696\n",
      "train loss:0.01898629218371701\n",
      "train loss:0.008431264548711513\n",
      "train loss:0.0070246998909752134\n",
      "train loss:0.009665695715713489\n",
      "train loss:0.02261529737131924\n",
      "train loss:0.020025238576028444\n",
      "train loss:0.00541055690447731\n",
      "train loss:0.013030749656637109\n",
      "train loss:0.0341836169647698\n",
      "train loss:0.02694817914369077\n",
      "train loss:0.01318182025108189\n",
      "train loss:0.037778496433352175\n",
      "train loss:0.011582668029437991\n",
      "train loss:0.006951928831204962\n",
      "train loss:0.012468695045544749\n",
      "train loss:0.020397927453274706\n",
      "train loss:0.02879673850579119\n",
      "train loss:0.01441656794230778\n",
      "train loss:0.014013504324754133\n",
      "train loss:0.01585357352591517\n",
      "train loss:0.00935444926320251\n",
      "train loss:0.02476465777313392\n",
      "train loss:0.01770021436485189\n",
      "train loss:0.009601267791673173\n",
      "train loss:0.010098852774911158\n",
      "train loss:0.012883514335587418\n",
      "train loss:0.00611261485031777\n",
      "train loss:0.015015462232221818\n",
      "train loss:0.04771822573110069\n",
      "train loss:0.00563612363523348\n",
      "train loss:0.007816827744015022\n",
      "train loss:0.010130489997000383\n",
      "train loss:0.012492948743158101\n",
      "train loss:0.009297424378886835\n",
      "train loss:0.00877189362116027\n",
      "train loss:0.0188622717193951\n",
      "train loss:0.00902208738211855\n",
      "train loss:0.008927029370978955\n",
      "train loss:0.007330695206602776\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.957\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 데이터셋 축소\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d07b433b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
