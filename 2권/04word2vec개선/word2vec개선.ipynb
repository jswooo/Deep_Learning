{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1rU-IlXYGT9K5KgdTzMejAdXZXx1Yg-np","authorship_tag":"ABX9TyMJsFKiJUlTmEyVQ+DntW/5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["%cd /content/drive/MyDrive/딥러닝2권/2권/04word2vec개선"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mS4SYC5C_5wB","executionInfo":{"status":"ok","timestamp":1712651778718,"user_tz":-540,"elapsed":523,"user":{"displayName":"조성우 (Aaron)","userId":"04985686994704365834"}},"outputId":"17cc4781-562d-43d4-fdb1-aaf6ba91df83"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/딥러닝2권/2권/04word2vec개선\n"]}]},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"LgbLGPUdAD8R","executionInfo":{"status":"ok","timestamp":1712651784053,"user_tz":-540,"elapsed":5,"user":{"displayName":"조성우 (Aaron)","userId":"04985686994704365834"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## 4.1 Embedding 계층"],"metadata":{"id":"Mzcva2Zk3Od1"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"NBWQR9Fi3GMF","executionInfo":{"status":"ok","timestamp":1712649751701,"user_tz":-540,"elapsed":6,"user":{"displayName":"조성우 (Aaron)","userId":"04985686994704365834"}}},"outputs":[],"source":["# 가중치 행렬에서 인덱스로 필요한 행만 가져오면 계산량을 줄일 수 있음\n","\n","class Embedding:\n","    def __init__(self, W):\n","        self.params = [W]\n","        self.grads = [np.zeros_like(W)]\n","        self.idx = None\n","\n","    def forward(self, idx):\n","        W = self.params\n","        self.idx = idx\n","        out = W[idx] # 필요 idx만\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dW = self.grads\n","        dW[...] = 0\n","\n","        for i, word_id in enumerate(self.idx):\n","            dW[word_id] += dout[i] # 중복 인덱스인 경우에는 합으로 역전파가 진행됨. (같은 값을 repeat하여 순전파했기에)\n","\n","        return None"]},{"cell_type":"markdown","source":["## 4.2 Negative Sampling"],"metadata":{"id":"nBpVUibL4dLB"}},{"cell_type":"code","source":["# 다중분류에서 이진분류로 변경해서 진행 -> softmax가 아닌 sigmoid를 활용\n","\n","# W_out을 이진분류 연산\n","class EmbeddingDot:\n","    def __init__(self, W):\n","        self.embed = Embedding(W)\n","        self.params = self.embed.params\n","        self.grads = self.embed.grads\n","        self.cache = None\n","\n","    def forward(self, h, idx):\n","        target_W = self.embed.forward(idx)\n","        out = np.sum(target_W * h, axis=1)\n","\n","        self.cache = (h, target_W)\n","\n","        return out\n","\n","    def backward(self, dout):\n","        h, target_W = self.cache\n","\n","        dout = dout.reshape(dout.shape[0],1)\n","\n","        dtarget_W = dout * h\n","        self.embed.backward(dtarget_W)\n","        dh = dout * target_W\n","\n","        return dh"],"metadata":{"id":"BMpfz5wg4fr0","executionInfo":{"status":"ok","timestamp":1712652059756,"user_tz":-540,"elapsed":4,"user":{"displayName":"조성우 (Aaron)","userId":"04985686994704365834"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# 확률 분포로 네거티브 샘플링에 활용할 샘플 가져오기\n","words = ['you', 'say', 'goodbye', 'I', 'hello', '.']\n","p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n","\n","print(np.random.choice(words, p=p))\n","\n","# 0.75를 제곱하여 확률을 좀 더 naive 하게 만들어줌(0.75가 아니어도 상관없음)\n","p = [0.7, 0.29, 0.01]\n","new_p = np.power(p, 0.75)\n","new_p /= np.sum(new_p)\n","print(new_p)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mr0A77yu4gQ8","executionInfo":{"status":"ok","timestamp":1712652344217,"user_tz":-540,"elapsed":3,"user":{"displayName":"조성우 (Aaron)","userId":"04985686994704365834"}},"outputId":"4f873c6f-b1b0-4745-a3fa-e953fe39a7d8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["you\n","[0.64196878 0.33150408 0.02652714]\n"]}]},{"cell_type":"code","source":["import collections\n","import sys\n","sys.path.append('..')\n","from common.np import *\n","\n","class UnigramSampler:\n","    def __init__(self, corpus, power, sample_size):\n","        self.sample_size = sample_size\n","        self.vocab_size = None\n","        self.word_p = None\n","\n","        counts = collections.Counter()\n","        for word_id in corpus:\n","            counts[word_id] += 1\n","\n","        vocab_size = len(counts)\n","        self.vocab_size = vocab_size\n","\n","        self.word_p = np.zeros(vocab_size)\n","        for i in range(vocab_size):\n","            self.word_p[i] = counts[i]\n","\n","        self.word_p = np.power(self.word_p, power)\n","        self.word_p /= np.sum(self.word_p)\n","\n","    def get_negative_sample(self, target):\n","        batch_size = target.shape[0]\n","\n","        if not GPU:\n","            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n","\n","            for i in range(batch_size):\n","                p = self.word_p.copy()\n","                target_idx = target[i]\n","                p[target_idx] = 0\n","                p /= p.sum()\n","                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n","        else:\n","            # GPU(cupy）로 계산할 때는 속도를 우선한다.\n","            # 부정적 예에 타깃이 포함될 수 있다.\n","            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n","                                               replace=True, p=self.word_p)\n","\n","        return negative_sample"],"metadata":{"id":"jTTcJy1rCDjX","executionInfo":{"status":"ok","timestamp":1712652624174,"user_tz":-540,"elapsed":1133,"user":{"displayName":"조성우 (Aaron)","userId":"04985686994704365834"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n","power = 0.75\n","sample_size = 2\n","\n","sampler = UnigramSampler(corpus, power, sample_size)\n","target = np.array([1, 3, 0])\n","negative_sample = sampler.get_negative_sample(target)\n","print(negative_sample)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tABsgWv0DUd3","executionInfo":{"status":"ok","timestamp":1712652871851,"user_tz":-540,"elapsed":5,"user":{"displayName":"조성우 (Aaron)","userId":"04985686994704365834"}},"outputId":"61e9f6a1-4bae-45c6-91b8-8308c11f7e73"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 3]\n"," [1 2]\n"," [3 2]]\n"]}]},{"cell_type":"code","source":["from common.layers import Embedding, SigmoidWithLoss\n","\n","# 네거티브 샘플링 로스 구현\n","class NegativeSamplingLoss:\n","    def __init__(self, W, corpus, power=0.75, sample_size=2):\n","        self.sample_size = sample_size\n","        self.sampler = UnigramSampler(corpus, power, sample_size)\n","        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)] # 긍정 1개 + 부정 개수만큼\n","        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n","\n","        self.params, self.grads = [], []\n","        for layer in self.embed_dot_layers:\n","            self.params += layer.params\n","            self.grads += layer.grads\n","\n","    def forward(self, h, target):\n","        batch_size = target.shape[0]\n","        negative_sample = self.sampler.get_negative_sample(target)\n","\n","        # positive\n","        score = self.embed_dot_layers[0].forward(h, target)\n","        correct_label = np.ones(batch_size, dtype=np.int32)\n","        loss = self.loss_layers[0].forward(score, correct_label)\n","\n","        # negative\n","        negative_label = np.zeros(batch_size, dtype=np.int32)\n","        for i in range(self.sample_size):\n","            negative_target = negative_sample[:, i]\n","            score = self.embed_dot_layers[i+1].forward(h, negative_target)\n","            loss += self.loss_layers[i+1].forward(score, negative_label) # 긍정 예와 부정 예의 loss 합으로 계산\n","\n","        return loss\n","\n","    def backward(self, dout=1):\n","        dh = 0\n","        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n","            dscore = l0.backward(dout)\n","            dh += l1.backward(dscore)\n","\n","        return dh"],"metadata":{"id":"D5ZwQrhwEZZv","executionInfo":{"status":"ok","timestamp":1712653703968,"user_tz":-540,"elapsed":1312,"user":{"displayName":"조성우 (Aaron)","userId":"04985686994704365834"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## 4.3 개선된 CBOW 모델 구현"],"metadata":{"id":"zsjWXLeDHaj-"}},{"cell_type":"code","source":["class CBOW:\n","    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n","        V, H = vocab_size, hidden_size\n","\n","        # 가중치 초기화\n","        w_in = 0.01 * np.random.randn(V, H).astype('f')\n","        w_out = 0.01 * np.random.randn(H, V).astype('f')\n","\n","        # layers\n","        self.in_layers = []\n","        for i in range(2 * window_size):\n","            layer = Embedding(w_in)\n","            self.in_layers.append(layer)\n","        self.ns_loss = NegativeSamplingLoss(w_out, corpus, power=0.75, sample_size=5)\n","\n","        # params and grads\n","        layers = self.in_layers + [self.ns_loss]\n","        self.params, self.grads = [], []\n","        for layer in layers:\n","            self.params += layer.params\n","            self.grads += layer.grads\n","\n","        # 분산 표현\n","        self.word_vecs = w_in\n","\n","    def forward(self, contexts, target):\n","        h = 0\n","        for i, layer in enumerate(self.in_layers):\n","            h += layer.forward(contexts[:, i])\n","        h *= 1 / len(self.in_layers) # 문맥의 평균값으로\n","        loss = self.ns_loss.forward(h, target)\n","\n","        return loss\n","\n","    def backward(self, dout=1):\n","        dout = self.ns_loss.backward(dout)\n","        dout *= 1 / len(self.in_layers)\n","        for layer in self.in_layers:\n","            layer.backward(dout)\n","\n","        return None"],"metadata":{"id":"e3W5QL_ZHdx0","executionInfo":{"status":"ok","timestamp":1712654818010,"user_tz":-540,"elapsed":379,"user":{"displayName":"조성우 (Aaron)","userId":"04985686994704365834"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["### CBOW 학습"],"metadata":{"id":"3uEqjndaMQqP"}},{"cell_type":"code","source":["from common import config\n","import pickle\n","from common.trainer import Trainer\n","from common.optimizer import Adam\n","from common.util import create_contexts_target, to_cpu, to_gpu\n","from dataset import ptb\n","\n","config.GPU = True # gpu 사용\n","\n","# 하이퍼파라미터\n","window_size = 5\n","hidden_size = 100\n","batch_size = 100\n","max_epoch = 10\n","\n","# data load\n","corpus, word_to_id, id_to_word = ptb.load_data('train')\n","vocab_size = len(word_to_id)\n","\n","contexts, target = create_contexts_target(corpus, window_size)\n","\n","if config.GPU:\n","    contexts, target = to_gpu(contexts), to_gpu(target)\n","\n","model = CBOW(vocab_size, hidden_size, window_size, corpus)\n","optimizer = Adam()\n","trainer = Trainer(model, optimizer)\n","\n","trainer.fit(contexts, target, max_epoch, batch_size)\n","trainer.plot()\n","\n","word_vecs = model.word_vecs\n","if config.GPU:\n","    word_vecs = to_cpu(word_vecs)\n","params = {}\n","params['word_vecs'] = word_vecs.astype(np.float16)\n","params['word_to_id'] = word_to_id\n","params['id_to_word'] = id_to_word\n","pkl_file = 'cbow_params.pkl'\n","with open(pkl_file, 'wb') as f:\n","    pickle.dump(params, f, -1)"],"metadata":{"id":"GhyO77KCLqUs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 모델 평가"],"metadata":{"id":"ydfQmJ16M6hS"}},{"cell_type":"code","source":["from common.util import most_similar\n","import pickle\n","\n","pkl_file = 'cbow_params.pkl'\n","\n","with open(pkl_file, 'rb') as f:\n","    params = pickle.load(f)\n","\n","    word_vecs = params['word_vecs']\n","    word_to_id = params['word_to_id']\n","    id_to_word = params['id_to_word']\n","\n","querys = ['you', 'year', 'car', 'toyota']\n","for query in querys:\n","    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"],"metadata":{"id":"-fNEpVTgM8lK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 복잡한 패턴 파악\n","def normalize(x):\n","    if x.ndim == 2:\n","        s = np.sqrt((x * x).sum(1))\n","        x /= s.reshape((s.shape[0], 1))\n","    elif x.ndim == 1:\n","        s = np.sqrt((x * x).sum())\n","        x /= s\n","    return x\n","\n","def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n","    for word in (a, b, c):\n","        if word not in word_to_id:\n","            print('%s(을)를 찾을 수 없습니다.' % word)\n","            return\n","\n","    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n","    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n","    query_vec = b_vec - a_vec + c_vec\n","    query_vec = normalize(query_vec)\n","\n","    similarity = np.dot(word_matrix, query_vec)\n","\n","    if answer is not None:\n","        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n","\n","    count = 0\n","    for i in (-1 * similarity).argsort():\n","        if np.isnan(similarity[i]):\n","            continue\n","        if id_to_word[i] in (a, b, c):\n","            continue\n","        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n","\n","        count += 1\n","        if count >= top:\n","            return\n","\n","analogy('king', 'man', 'queen', word_to_id, id_to_word, word_vecs)\n","analogy('take', 'took', 'go', word_to_id, id_to_word, word_vecs)\n","analogy('car', 'cars', 'child', word_to_id, id_to_word, word_vecs)\n","analogy('good', 'better', 'bad', word_to_id, id_to_word, word_vecs)"],"metadata":{"id":"uXgDUg3tN4Db"},"execution_count":null,"outputs":[]}]}